{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c046ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from collections import OrderedDict\n",
    "from IPython.display import Audio\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import librosa\n",
    "import numpy as np\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a26d93ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 16354it [00:00, 29711.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading transcripts for language: English. Error: Unable to allocate 1.61 MiB for an array with shape (210816, 1) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 15520it [00:00, 24191.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading transcripts for language: Spanish. Error: Unable to allocate 3.34 MiB for an array with shape (437184,) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 9630it [00:00, 18190.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading transcripts for language: Russian. Error: Unable to allocate 2.15 MiB for an array with shape (281664,) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 4604it [00:00, 22022.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading transcripts for language: Japanese. Error: Unable to allocate 2.44 MiB for an array with shape (319680, 1) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 10581it [00:00, 32852.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading transcripts for language: Mandarin. Error: Unable to allocate 2.29 MiB for an array with shape (299760,) and data type float64\n",
      "      language                                           sentence  \\\n",
      "0      English  Joe Keaton disapproved of films, and Buster al...   \n",
      "1      English                               She'll be all right.   \n",
      "2      English                                                six   \n",
      "3      English                         All's well that ends well.   \n",
      "4      English  It is a busy market town that serves a large s...   \n",
      "...        ...                                                ...   \n",
      "2462  Mandarin                                       殿试登进士第二甲第四名。   \n",
      "2463  Mandarin                                     他主要的工作是研究冷凝气体。   \n",
      "2464  Mandarin                                            属米兰总教区。   \n",
      "2465  Mandarin                                               张九皋。   \n",
      "2466  Mandarin                                       小酒吧老板天民家中休息。   \n",
      "\n",
      "                                                  audio  \n",
      "0     {'path': 'common_voice_en_27710027.mp3', 'arra...  \n",
      "1     {'path': 'common_voice_en_699711.mp3', 'array'...  \n",
      "2     {'path': 'common_voice_en_21953345.mp3', 'arra...  \n",
      "3     {'path': 'common_voice_en_18132047.mp3', 'arra...  \n",
      "4     {'path': 'common_voice_en_27340672.mp3', 'arra...  \n",
      "...                                                 ...  \n",
      "2462  {'path': 'common_voice_zh-CN_22122977.mp3', 'a...  \n",
      "2463  {'path': 'common_voice_zh-CN_22115447.mp3', 'a...  \n",
      "2464  {'path': 'common_voice_zh-CN_22102867.mp3', 'a...  \n",
      "2465  {'path': 'common_voice_zh-CN_22217555.mp3', 'a...  \n",
      "2466  {'path': 'common_voice_zh-CN_22450333.mp3', 'a...  \n",
      "\n",
      "[2467 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of language codes for the desired languages\n",
    "\n",
    "\n",
    "language_codes = {\n",
    "    \"en\": \"English\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"zh-CN\": \"Mandarin\",\n",
    "}\n",
    "\n",
    "transcripts = []\n",
    "\n",
    "for lang_code, lang_name in language_codes.items():\n",
    "    try:\n",
    "        dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", lang_code, split=\"test\", streaming=True)\n",
    "        count = 0\n",
    "        for x in dataset:\n",
    "            if count >= 5000:\n",
    "                break\n",
    "            count += 1\n",
    "            transcripts.append({\"language\": lang_name, \"sentence\": x[\"sentence\"], \"audio\": x[\"audio\"]})\n",
    "        print(f\"Loaded transcripts for language: {lang_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading transcripts for language: {lang_name}. Error: {str(e)}\")\n",
    "\n",
    "df = pd.DataFrame(transcripts)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db86e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Romanize the transcripts\n",
    "\n",
    "def romanize_text(text):\n",
    "    romanized_text = unidecode(text)\n",
    "    return romanized_text.strip()\n",
    "\n",
    "# Clean transcripts in the DataFrame\n",
    "df['sentence'] = df['sentence'].apply(romanize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab93a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def extract_features(signal, sr):\n",
    "    # Step 1: Preprocessing\n",
    "    signal = librosa.effects.trim(signal)[0]\n",
    "    \n",
    "    # Step 2: Frame the signal\n",
    "    hop_length = int(sr / 100) # 10ms hop length\n",
    "    frames = librosa.util.frame(signal, frame_length=hop_length, hop_length=hop_length)\n",
    "    \n",
    "    # Step 3: Apply a window function\n",
    "    window = np.hamming(hop_length)\n",
    "    frames_windowed = frames * window[:, np.newaxis]\n",
    "    \n",
    "    # Step 4: Compute the spectral features\n",
    "    stft = np.abs(librosa.stft(signal, n_fft=hop_length*2, hop_length=hop_length, win_length=hop_length*2, window='hamming'))\n",
    "    mel = librosa.feature.melspectrogram(sr=sr, S=stft**2)\n",
    "    mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel))\n",
    "    \n",
    "    # Step 5: Aggregate the features\n",
    "    features = np.concatenate([\n",
    "        np.mean(mfcc, axis=1),\n",
    "        np.std(mfcc, axis=1),\n",
    "        np.median(mfcc, axis=1),\n",
    "    ])\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b4a67c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2467\n",
      "2467\n",
      "2467\n"
     ]
    }
   ],
   "source": [
    "X_text = []\n",
    "X_audio = []\n",
    "y = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    preprocessed_sentence = preprocess_text(row['sentence'])\n",
    "    X_text.append(preprocessed_sentence)\n",
    "    y.append(row['language'])\n",
    "    feature = extract_features(row['audio']['array'], row['audio']['sampling_rate'])\n",
    "    X_audio.append(feature)\n",
    "    \n",
    "print(len(X_text))\n",
    "print(len(X_audio))\n",
    "print(len(y))\n",
    "X_textTrain, X_textTest, X_audioTrain, X_audioTest, y_train, y_test = train_test_split(X_text, X_audio, y, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5761a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_textTrain = tfidf.fit_transform(X_textTrain)\n",
    "X_textTest = tfidf.transform(X_textTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d03141fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9176788124156545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       0.92      1.00      0.96       682\n",
      "    Japanese       0.00      0.00      0.00         1\n",
      "    Mandarin       1.00      0.02      0.04        45\n",
      "     Russian       0.00      0.00      0.00         5\n",
      "     Spanish       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.92       741\n",
      "   macro avg       0.38      0.20      0.20       741\n",
      "weighted avg       0.91      0.92      0.88       741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusta\\.conda\\envs\\pythonProject5\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gusta\\.conda\\envs\\pythonProject5\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gusta\\.conda\\envs\\pythonProject5\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "audioModel = SVC(kernel='linear', C=1, gamma='scale')\n",
    "audioModel.fit(X_audioTrain, y_train)\n",
    "\n",
    "# Test the model on the testing set\n",
    "y_pred = audioModel.predict(X_audioTest)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "y_svc_predicted = audioModel.predict(X_audioTest)\n",
    "print(classification_report(y_test, y_svc_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "475e2b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9689608636977058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       0.97      1.00      0.98       682\n",
      "    Japanese       0.00      0.00      0.00         1\n",
      "    Mandarin       0.97      0.78      0.86        45\n",
      "     Russian       0.00      0.00      0.00         5\n",
      "     Spanish       1.00      0.12      0.22         8\n",
      "\n",
      "    accuracy                           0.97       741\n",
      "   macro avg       0.59      0.38      0.41       741\n",
      "weighted avg       0.96      0.97      0.96       741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gusta\\.conda\\envs\\pythonProject5\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gusta\\.conda\\envs\\pythonProject5\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gusta\\.conda\\envs\\pythonProject5\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "textModel = LogisticRegression()\n",
    "textModel.fit(X_textTrain, y_train)\n",
    "\n",
    "\n",
    "y_pred = textModel.predict(X_textTest)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "y_lr_predicted = textModel.predict(X_textTest)\n",
    "print(classification_report(y_test, y_lr_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
